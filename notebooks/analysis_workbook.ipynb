{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from matplotlib.pyplot import subplots\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "database_password = os.environ.get(\"DATABASE_PASSWORD\")\n",
    "database_username = os.environ.get(\"DATABASE_USERNAME\")\n",
    "database_host = os.environ.get(\"DATABASE_HOST\")\n",
    "database_port = os.environ.get(\"DATABASE_PORT\")\n",
    "database_name = os.environ.get(\"DATABASE_NAME\")\n",
    "\n",
    "connection = psycopg2.connect(database=database_name,\n",
    "                        host=database_host,\n",
    "                        user=database_username,\n",
    "                        password=database_password,\n",
    "                        port=database_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_to_dataframe(conn, query) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Import data from a PostgreSQL database using a SELECT query.\n",
    "    Based on https://medium.com/@alestamm/importing-data-from-a-postgresql-database-to-a-pandas-dataframe-5f4bffcd8bb2\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(f\"Error: {error}â€\")\n",
    "        cursor.close()\n",
    "        return 1\n",
    "    # The execute returns a list of tuples:\n",
    "    tuples_list = cursor.fetchall()\n",
    "    colnames = [desc[0] for desc in cursor.description]\n",
    "    cursor.close()\n",
    "    # Now we need to transform the list into a pandas DataFrame:\n",
    "    df = pd.DataFrame(tuples_list, columns=colnames)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jobs live, including historic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_live_df = sql_to_dataframe(conn=connection, query=\"\"\"SELECT DATE(checked_time), COUNT(DISTINCT scrape_id)\n",
    "\tFROM (SELECT * FROM sitemap_entries\n",
    "\t\tWHERE checked_time > '2024-01-10' AND NOT checked_time = '2024-02-10 01:36:09.964627+00' AND NOT url = 'https://www.civilservicejobs.service.gov.uk/csr/index.cgi'\n",
    "\t\tORDER BY checked_time ASC) m\n",
    "GROUP BY DATE(checked_time)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_live_df['date'] = pd.to_datetime(jobs_live_df['date'])\n",
    "jobs_live_df.set_index(\"date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_scrapes = pd.read_csv(\"../src/data/historic_scrapes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_scrapes[\"date\"] = pd.to_datetime(historic_scrapes[\"date\"], dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_scrapes.set_index(\"date\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_live_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historic_scrapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([jobs_live_df, historic_scrapes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = combined_df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize=(12,6))\n",
    "\n",
    "ax.plot(combined_df.index, combined_df[\"count\"])\n",
    "\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "ax.set_title(\"Number of jobs live on Civil Service Jobs\")\n",
    "\n",
    "ax.set_ylabel(\"Number of jobs live\")\n",
    "\n",
    "#ax.set_xticks(combined_df[\"date\"])\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator())\n",
    "ax.tick_params(axis='x', rotation=70)\n",
    "\n",
    "fig.subplots_adjust(bottom=0.27)\n",
    "\n",
    "fig.text(0.5, 0.01, \"[Quick, non peer-reviewed visualisation based on unofficial once-daily web scraping since 2024-01-11. It may be incorrect. \\n Measurements before then obtained using Archive.org. \\n Only 'external' jobs (available to all, not only existing civil servants) are included. \\n Civil Service Jobs data is available under the Open Government Licence v3.0]\", ha=\"center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jobs live on Wednesdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_live_df = sql_to_dataframe(conn=connection, query=\"\"\"SELECT DATE(checked_time), COUNT(DISTINCT scrape_id)\n",
    "\tFROM (SELECT * FROM sitemap_entries\n",
    "\t\tWHERE checked_time > '2024-01-10' AND NOT checked_time = '2024-02-10 01:36:09.964627+00' AND NOT url = 'https://www.civilservicejobs.service.gov.uk/csr/index.cgi'\n",
    "\t\tORDER BY checked_time ASC) m\n",
    "GROUP BY DATE(checked_time)\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_live_df['date'] = pd.to_datetime(jobs_live_df['date'])\n",
    "\n",
    "# Filter the DataFrame to only include Wednesdays (where dayofweek == 2)\n",
    "wednesdays_df = jobs_live_df[jobs_live_df['date'].dt.dayofweek == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize=(10,6))\n",
    "\n",
    "ax.plot(wednesdays_df[\"date\"], wednesdays_df[\"count\"], marker=\"+\", markeredgecolor=\"red\")\n",
    "\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "ax.set_title(\"Number of jobs live on Civil Service Jobs\")\n",
    "\n",
    "ax.set_ylabel(\"Number of jobs live\")\n",
    "\n",
    "ax.set_xticks(wednesdays_df[\"date\"])\n",
    "ax.tick_params(axis='x', rotation=70)\n",
    "\n",
    "fig.subplots_adjust(bottom=0.28)\n",
    "\n",
    "fig.text(0.5, 0.01, \"[Quick, non peer-reviewed visualisation based on unofficial once-daily web scraping. \\n Only 'external' jobs (available to all, not only existing civil servants) are included. \\n Civil Service Jobs data is available under the Open Government Licence v3.0]\", ha=\"center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total new jobs added per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earliest_times_per_jcode_df = sql_to_dataframe(conn=connection, query=\"\"\"SELECT url, COUNT(DISTINCT scrape_id), MIN(checked_time) AS earliest_checked_time, MIN(updated_time) AS earliest_updated_time \n",
    "                                               FROM sitemap_entries \n",
    "                                               WHERE checked_time > '2024-01-10' AND NOT checked_time = '2024-02-10 01:36:09.964627+00' AND NOT url = 'https://www.civilservicejobs.service.gov.uk/csr/index.cgi'\n",
    "                                               GROUP BY url\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earliest_times_per_jcode_df[\"earliest_updated_time\"] = pd.to_datetime(earliest_times_per_jcode_df[\"earliest_updated_time\"], utc=True)\n",
    "\n",
    "df_counts_by_date = earliest_times_per_jcode_df.groupby(earliest_times_per_jcode_df[\"earliest_updated_time\"].dt.date).size().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_by_date[\"earliest_updated_time\"] = pd.to_datetime(df_counts_by_date[\"earliest_updated_time\"])\n",
    "df_counts_by_date = df_counts_by_date[df_counts_by_date[\"earliest_updated_time\"] >= pd.to_datetime(\"2024-01-11\")]\n",
    "df_counts_by_date.set_index(\"earliest_updated_time\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_by_date = df_counts_by_date.asfreq('D', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize=(10,6))\n",
    "\n",
    "ax.plot(df_counts_by_date.index, df_counts_by_date[\"count\"])\n",
    "\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "ax.set_title(\"Number of new jobs posted on Civil Service Jobs per day\")\n",
    "\n",
    "ax.set_ylabel(\"Number of new jobs posted\")\n",
    "\n",
    "fig.subplots_adjust(bottom=0.18)\n",
    "\n",
    "fig.text(0.5, 0.01, \"[Quick, non peer-reviewed visualisation based on unofficial once-daily web scraping. \\n Only 'external' jobs (available to all, not only existing civil servants) are included. \\n Civil Service Jobs data is available under the Open Government Licence v3.0]\", ha=\"center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total new jobs added per week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earliest_times_per_jcode_df_for_week = sql_to_dataframe(conn=connection, query=\"\"\"SELECT url, COUNT(DISTINCT scrape_id), MIN(checked_time) AS earliest_checked_time, MIN(updated_time) AS earliest_updated_time \n",
    "                                               FROM sitemap_entries \n",
    "                                               WHERE checked_time > '2024-01-10' AND NOT checked_time = '2024-02-10 01:36:09.964627+00' AND NOT url = 'https://www.civilservicejobs.service.gov.uk/csr/index.cgi'\n",
    "                                               GROUP BY url\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earliest_times_per_jcode_df_for_week[\"earliest_updated_time\"] = pd.to_datetime(earliest_times_per_jcode_df_for_week[\"earliest_updated_time\"], utc=True)\n",
    "df_counts_by_week = earliest_times_per_jcode_df_for_week.groupby(earliest_times_per_jcode_df_for_week[\"earliest_updated_time\"].dt.to_period('W')).size().to_timestamp().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_by_week[\"earliest_updated_time\"] = pd.to_datetime(df_counts_by_week[\"earliest_updated_time\"])\n",
    "df_counts_by_week = df_counts_by_week[df_counts_by_week[\"earliest_updated_time\"] >= pd.to_datetime(\"2024-01-15\")]\n",
    "df_counts_by_week = df_counts_by_week[df_counts_by_week[\"earliest_updated_time\"] <= pd.to_datetime(\"2024-05-19\")] # Set to the Sunday at the end of last week want to include\n",
    "df_counts_by_week.set_index(\"earliest_updated_time\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_counts_by_week = df_counts_by_week.asfreq('W-MON', fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = subplots(figsize=(16,6))\n",
    "\n",
    "ax.plot(df_counts_by_week.index, df_counts_by_week[\"count\"], marker=\"+\", markeredgecolor=\"red\")\n",
    "\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "ax.set_title(\"Number of new jobs posted on Civil Service Jobs per week\")\n",
    "\n",
    "ax.set_ylabel(\"Number of new jobs posted\")\n",
    "\n",
    "ax.set_xticks(df_counts_by_week.index)\n",
    "\n",
    "n = 2  # Keeps every nth label. From https://stackoverflow.com/questions/20337664/cleanest-way-to-hide-every-nth-tick-label-in-matplotlib-colorbar\n",
    "[l.set_visible(False) for (i,l) in enumerate(ax.xaxis.get_ticklabels()) if i % n != 0]\n",
    "\n",
    "fig.subplots_adjust(bottom=0.18)\n",
    "\n",
    "fig.text(0.5, 0.01, \"(Quick, non peer-reviewed visualisation based on unofficial once-daily web scraping. \\n Only 'external' jobs (available to all, not only existing civil servants) are included. \\n Civil Service Jobs data is available under the Open Government Licence v3.0)\", ha=\"center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jobs live on Wednesdays, for a specific department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "department_name = \"Department for Environment, Food and Rural Affairs\"\n",
    "\n",
    "jobs_live_for_dept_df = sql_to_dataframe(conn=connection, query=f\"\"\"SELECT\n",
    "\tDATE (CHECKED_TIME),\n",
    "\tCOUNT(DISTINCT URL)\n",
    "FROM\n",
    "\t(\n",
    "\t\tSELECT\n",
    "\t\t\tMAIN.URL,\n",
    "\t\t\tMAIN.CHECKED_TIME,\n",
    "\t\t\tDEPARTMENTS.DEPARTMENT_NAME\n",
    "\t\tFROM\n",
    "\t\t\t(\n",
    "\t\t\t\tSELECT\n",
    "\t\t\t\t\tFILTERED_SITEMAP_ENTRIES.*,\n",
    "\t\t\t\t\tDEPARTMENT_ID_FOR_EACH_SCRAPE_ID.DEPARTMENT_ID\n",
    "\t\t\t\tFROM\n",
    "\t\t\t\t\t(\n",
    "\t\t\t\t\t\tSELECT\n",
    "\t\t\t\t\t\t\t*\n",
    "\t\t\t\t\t\tFROM\n",
    "\t\t\t\t\t\t\tSITEMAP_ENTRIES\n",
    "\t\t\t\t\t\tWHERE\n",
    "\t\t\t\t\t\t\tCHECKED_TIME > '2024-01-10'\n",
    "\t\t\t\t\t\t\tAND NOT CHECKED_TIME = '2024-02-10 01:36:09.964627+00'\n",
    "\t\t\t\t\t\t\tAND NOT URL = 'https://www.civilservicejobs.service.gov.uk/csr/index.cgi'\n",
    "\t\t\t\t\t) FILTERED_SITEMAP_ENTRIES\n",
    "\t\t\t\t\tLEFT JOIN (\n",
    "\t\t\t\t\t\tSELECT\n",
    "\t\t\t\t\t\t\tSCRAPE_ID,\n",
    "\t\t\t\t\t\t\tDEPARTMENT_ID\n",
    "\t\t\t\t\t\tFROM\n",
    "\t\t\t\t\t\t\tSCRAPES_EXTRACTED\n",
    "\t\t\t\t\t) DEPARTMENT_ID_FOR_EACH_SCRAPE_ID ON FILTERED_SITEMAP_ENTRIES.SCRAPE_ID = DEPARTMENT_ID_FOR_EACH_SCRAPE_ID.SCRAPE_ID\n",
    "\t\t\t) MAIN\n",
    "\t\t\tLEFT JOIN DEPARTMENTS ON MAIN.DEPARTMENT_ID = DEPARTMENTS.DEPARTMENT_ID\n",
    "\t\tWHERE\n",
    "\t\t\tDEPARTMENT_NAME = '{department_name}'\n",
    "\t) SITEMAP_ENTRIES_WITH_DEPARTMENT_NAME\n",
    "GROUP BY\n",
    "\tDATE (CHECKED_TIME)\"\"\")\n",
    "\n",
    "jobs_live_for_dept_df['date'] = pd.to_datetime(jobs_live_for_dept_df['date'])\n",
    "\n",
    "jobs_live_for_dept_df.set_index(\"date\", inplace=True)\n",
    "jobs_live_for_dept_df = jobs_live_for_dept_df.asfreq(\"D\", fill_value=0)\n",
    "\n",
    "# Filter the DataFrame to only include Wednesdays (where dayofweek == 2)\n",
    "jobs_live_for_dept_wednesdays_df = jobs_live_for_dept_df[jobs_live_for_dept_df.index.dayofweek == 3]\n",
    "\n",
    "fig, ax = subplots(figsize=(10,6))\n",
    "\n",
    "ax.plot(jobs_live_for_dept_wednesdays_df.index, jobs_live_for_dept_wednesdays_df[\"count\"], marker=\"+\", markeredgecolor=\"red\")\n",
    "\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "ax.set_title(f\"Number of jobs live on Civil Service Jobs from '{department_name}'\")\n",
    "\n",
    "ax.set_ylabel(\"Number of jobs live\")\n",
    "\n",
    "ax.set_xticks(jobs_live_for_dept_wednesdays_df.index)\n",
    "ax.tick_params(axis='x', rotation=70)\n",
    "\n",
    "fig.subplots_adjust(bottom=0.35)\n",
    "\n",
    "fig.text(0.5, 0.01, \"As recorded at 1AM. Dates plotted are Thursdays. \\n\\n[This is a quick, non peer-reviewed visualisation based on unofficial once-daily web scraping. It may be incorrect. \\n Only 'external' jobs (available to all, not only existing civil servants) are included. \\n Civil Service Jobs data is available under the Open Government Licence v3.0]\", ha=\"center\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
